

========================================
FILE: 00-overview.md
========================================

# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Overview

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

## Контекст (связь с текущим кодом)

На дату `2026-01-20` фактический флоу описан в `../chat-flow-from-code-2026-01-20.md`.

Текущее состояние (важно для совместимости):

- Backend собирает prompt и стримит SSE (`llm.stream.*`).
- `pipeline_runs` / `pipeline_step_runs` уже существуют, но шаги фиксированы (`pre` + `llm`) и выступают как лог.
- Канонический promptable текст для LLM сейчас фактически берётся из `promptText` (denorm кэш выбранного варианта).
- `blocksJson` присутствует в схеме, но почти не участвует в генерации/стриме.

Эта спека описывает **целевую архитектуру v1**, которая “доразвивает” текущий подход без ломки инвариантов.

## Цели

- **Backend-first orchestration**: сервер является источником правды о том, что пошло в LLM и как получился результат.
- **Наблюдаемость**: уметь объяснить “почему такой ответ” через `pipeline_runs`, `pipeline_step_runs`, `llm_generations` и (опционально) snapshot prompt.
- **Расширяемость**: заложить архитектуру под RAG/tool/post-processing, не переписывая историю в БД и не усложняя UI.  
  Уточнение: в v1 **реализуем** pre/post вокруг существующего chat-core, а `rag`/`tool` **только резервируем** (план на v2+).
- **Безопасная гибкость**: пайплайны могут влиять на prompt и результат текущего ответа, но влияние должно быть контролируемым (см. policy мутаций).

## Non-goals (v1)

- Полный DAG/граф зависимостей шагов (в v1 — линейная последовательность).
- Исторические “переписывания” чата (compaction/summary с заменой сообщений) по умолчанию.
- Потоковый (streaming) рендер и инкрементальное обновление `blocksJson` как обязательная фича.
- Полноценная реализация `rag`/`tool` шагов (retrieval/tools инфраструктуры в проекте пока нет; это план v2+).
- Сложные tool↔LLM циклы/петли (agent loop) как must-have.  
  При этом **aux LLM-вызовы пайплайна** (один или несколько) внутри одного `PipelineRun` допустимы, но они не являются “main llm” и не меняют его механику (см. `10-execution-model.md`).

## Термины

- **Pipeline**: описание процесса (набор шагов + настройки + условия), который выполняется при определённом триггере.
- **PipelineProfile**: сохраняемая конфигурация (набор “активных” пайплайнов), которую пользователь может:
  - выбрать как глобальный default,
  - закрепить (override) на уровне `entityProfile` и/или конкретного `chat`.
  PipelineProfile — это **конфиг**, а не выполнение.
- **Active PipelineProfile**: PipelineProfile, который фактически используется для данного `chat` (резолвится по правилу приоритетов: `chat override` → `entityProfile override` → `global default`).
- **PipelineRun**: конкретный запуск Pipeline.
- **PipelineStepRun**: выполнение одного шага внутри PipelineRun, с логом входа/выхода/ошибок/тайминга.
- **Step**: атомарная стадия исполнения, например `pre`, `rag`, `llm`, `post`, `tool`.
- **Generation**: попытка получить/обновить текст ассистента через LLM (стрим/аборт/метрики).
- **PromptDraft**: “рабочий” prompt, который реально будет отправлен в LLM (system + history + prompt-time инъекции/augmentation + policy), но не обязан быть БД-сущностью.  
  Примечание: `rag`/`tool` как источники инъекций зарезервированы на v2+.
- **OutputDraft**: “рабочий” результат (накопленный assistant text + blocks/meta), который будет сохранён в БД в конце.
- **Mutation**: изменение состояния, которое шаг(и) пайплайна пытаются применить (к prompt-time или к каноническому состоянию).
- **Session (Artifact Session)**: “место жизни” persisted-артефактов для пайплайнов в рамках конкретного чата.
  - v1: сессия **строго chat-scoped** (ключ `chatId`), ветки не учитываем ради упрощения.
  - сессия — это persisted состояние (в БД), но в рантайме materialize’ится в `SessionView` (словарь `tag -> { value, history, meta }`) для шагов и Liquid.
- **PipelineArtifact**: универсальная единица “вывода/контента/памяти”, созданная пайплайном. Артефакт может:
  - отображаться в UI в разных местах (`uiSurface`),
  - участвовать или не участвовать в prompt (`visibility`),
  - жить как “эпизодический” (на один run) или “долговременный” (retention).
- **ArtifactAccess**: доступность артефакта для других пайплайнов/шаблонов:
  - `run_only` — существует только внутри текущего `PipelineRun` (не адресуется как `art.<tag>`, не требует уникального `tag`)
  - `persisted` — сохраняется и доступен другим пайплайнам/шаблонам по `tag` через `art.<tag>` (tag уникален)
- **ArtifactVisibility**: участие артефакта в prompt/UI:
  - `prompt_only` — участвует в prompt, не показывается пользователю
  - `ui_only` — показывается пользователю, не участвует в prompt
  - `prompt_and_ui` — и участвует в prompt, и показывается
  - `internal` — не участвует в prompt и не показывается в UI; может быть:
    - `access=run_only` (внутренний промежуточный результат шага),
    - или `access=persisted` (служебный результат для других пайплайнов/валидаторов/тул-обработки, но не для UI/prompt).
- **UiSurface**: “поверхность”/место отображения артефакта:
  - `chat_history` — внутри истории чата (как сообщение/блок)
  - `panel:<id>` — отдельный UI-виджет/панель (например `panel:rpg_state`)
  - `feed:<id>` — отдельная лента/поток (например `feed:commentary`)
  - `overlay:<id>` — оверлей/всплывающая поверхность (опционально)
- **Augmentation**: частный случай артефакта: промежуточный “prompt-only” контент (план/скретчпад/CoT-подобная заготовка), который используется **только для одной** последующей генерации и **не сохраняется** как сообщение истории.
- **PipelineState**: частный случай артефакта: долговременное состояние/память (например: “игровое время”, “локация”, “активные персонажи”), доступное другим шагам/пайплайнам как `art.<tag>`.

## Инварианты (обязательные правила v1)

### Backend source of truth

- Истина истории — в БД: `chat_messages` + выбранный `message_variants` (если включены).
- Истина процесса — в БД: `pipeline_runs`, `pipeline_step_runs`, `llm_generations`.
- Frontend не формирует финальный prompt (может показывать локально optimistic UI, но не решает политику контекста).

Уточнение v1 (совместимость с текущим chat-core):

- “Вызов LLM”/стриминг генерации в v1 рассматривается как **существующий механизм** backend-а (chat-core), который мы не переписываем ради пайплайнов.
- Пайплайны в v1 добавляют **обвязку** вокруг генерации: `pre` до вызова и `post` после `done`, не меняя протокол стрима и базовую механику flush в БД.

### Promptable truth: `promptText`

- Канонический “promptable” слой истории в v1 берётся из **`promptText` выбранного состояния** сообщений.
- При этом поверх истории допускаются **prompt-time** модификации в `PromptDraft` (augmentation / system notes / prompt-only replace и т.п.; RAG/tool — v2+), которые **не становятся частью истории**.
- `blocksJson` предназначен для UI и расширений; он может не совпадать с `promptText`.

> Дополнение (перепроектирование ради расширяемости): если мы вводим `PipelineArtifact` как first-class сущность,
> то `blocksJson` становится “быстрым UI-представлением” для `uiSurface=chat_history` (denorm),
> а источником правды о том, что создано пайплайном, становятся артефакты.

Уточнение (важно для кейса “pipeline переписывает user-сообщение”):

- Инвариант `promptText` остаётся: в prompt попадает **только `promptText` выбранного состояния**.
- Но “выбранное состояние” может быть:
  - исходным user-вводом (как есть),
  - **вариантом** user-сообщения (например `kind=normalized|rewritten`), выбранным сервером по policy пайплайна,
  - либо prompt-time проекцией (если это не должно становиться каноном истории).

### Private reasoning / CoT

- В системе допускаются “скретчпад/CoT-подобные” артефакты для улучшения качества, но они считаются **private/internal**:
  - **не показываются пользователю**,
  - **не попадают в `promptText` истории**,
  - по умолчанию **не логируются как сырой текст** в БД (только redacted/сводка/хеш), чтобы не хранить лишнее и не утекать.
- Практическая рекомендация: просить модель не “raw chain-of-thought”, а структурированный **план/заметки** (“working notes”) в формате, который безопасно использовать как `Augmentation`.

### Ограничение мутаций истории

- В v1 пайплайны **не переписывают прошлую историю** как часть обычного “одного хода” (turn).
- Допускаются только:
  - **prompt-time трансформации** (влияние на PromptDraft без изменения БД истории),
  - **каноническое сохранение** результата текущего ответа (assistant variant/message),
  - отдельные явно выделенные операции (см. “Исторические трансформации”, v2+ или opt-in).

## Уточнение: `branchId` vs chat-scoped Session (v1)

UI может иметь представление “веток”, но в v1 это сознательное упрощение:

- persisted-артефакты живут в **одной сессии на чат** (`sessionId = chatId`) и **не разделяются по `branchId`**.
- `branchId` остаётся в `PipelineRunContext` как часть корреляции/совместимости и будущего расширения, но **не влияет** на storage артефактов в v1.
- v2+ (опционально): возможен переход к `sessionScope = branch` или гибридной модели, если ветки станут обязательными для UX пайплайнов.



========================================
FILE: 10-execution-model.md
========================================

# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Execution model

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

## Pipeline execution model (v1)

### Триггеры запусков

Минимальный набор:

- `user_message`: пользователь отправил новое сообщение
- `regenerate`: регенерация варианта для assistant message
- `manual`: ручной запуск (кнопка/endpoint) — опционально
- `api`: внешний вызов API — опционально

### Idempotency и ретраи (v1 минимум)

В v1 фиксируем “минимум, который защищает от дублей”, без сложной идемпотентности API:

- Для одного “turn” в рамках чата должен существовать **только один** канонический запуск пайплайна:
  - trigger `user_message`: ключ дедупликации = `(chatId, userMessageId)`
  - trigger `regenerate`: ключ дедупликации = `(chatId, assistantVariantId)` (или эквивалентный стабильный идентификатор “перегенерации”)
- Если сервер получает повторный запрос с тем же ключом, он **не создаёт** новый `PipelineRun/Generation`, а возвращает уже существующие идентификаторы и текущий статус.

Политика ретраев (v1):

- Автоматических ретраев пайплайна/LLM по умолчанию **нет**.
- Повтор попытки — это явный пользовательский `regenerate` (или новый `user_message`).

### Высокоуровневый flow “одного хода”

1. Backend сохраняет user message (если trigger = `user_message`).
2. Backend создаёт assistant message + assistant variant (пустые) и создаёт `PipelineRun`.
3. Pipeline выполняет шаги (линейно): `pre` → (`rag`?) → `llm` → (`post`?).
4. `llm` шаг создаёт `Generation` и стримит delta события.
5. По завершению (done/abort/error) выполняется финализация:
   - финальный flush в БД,
   - `post` (если есть),
   - обновление статусов `pipeline_runs`, `pipeline_step_runs`, `llm_generations`.

### Уточнение v1: `llm` как “boundary” существующей генерации

В v1 `llm` — это не “настраиваемый плагин”, а **точка вызова уже существующего механизма генерации** (chat-core):

- текущая реализация стрима SSE и периодического flush текста в БД считается “как есть”,
- пайплайн в v1 выполняет только подготовку **до** вызова (`pre`) и обработку **после** завершения (`post`),
- управление выбором провайдера/модели/протокола стрима внутри `llm` шага **не является целью v1** (мы сознательно не ломаем то, что уже работает).

Единственный допустимый “вход” в генерацию в v1 — passthrough `settings` из запроса пользователя (opaque json), совместимый с текущим генератором.

### Порядок пайплайнов и единственный `main llm` (уточнение v1)

В v1 фиксируем упрощение, важное для UX и детерминизма:

- **`main llm` строго один** “канонический” процесс стрима/сохранения на момент времени (в рамках ветки/чата).  
  Он зависит от template/сборки prompt и является единственным источником `llm.stream.*` для `chat_history`.
- Порядок запуска пайплайнов определяется **позицией в `PipelineProfile`**:
  - пайплайн “выше” выполняется раньше,
  - порядок шагов внутри пайплайна — как описано в definition (линейно).

Ограничение v1 (на момент реализации):

- Прямые “конкурентные” правки одного `PromptDraft` несколькими пайплайнами/шагами **не поддерживаем** в v1 (тема v2+).
  - В v1 композиция влияний нескольких пайплайнов на effective prompt делается **только через `PipelineArtifact` + `promptInclusion`**:
    - шаги создают/обновляют артефакты,
    - оркестратор собирает итоговый `PromptDraft` из канонической history (`promptText`) + systemPrompt + включённых артефактов,
    - порядок определяется детерминированно (см. `40-artifacts.md`: PipelineProfile order → step order → tie-break).

### Параллельное исполнение и очередь коммитов (v1)

Мы допускаем, что часть пайплайнов/шагов может выполняться **параллельно** (например валидация, расчёты; RAG/tool — v2+),
но изменения, которые влияют на **историю чата и UI-поток сообщений**, должны применяться **детерминированно**.

Модель v1:

- **Execute (можно параллельно)**:
  - шаги/пайплайны выполняют вычисления,
  - создают/обновляют `PipelineArtifact` (особенно `internal`/`ui_only`/`prompt_only`),
  - готовят “предложения изменений” (patch/intent), но не коммитят их в историю чата.

- **Commit (строго последовательно)**:
  - для каждого `(chatId, branchId)` существует **очередь коммитов**,
  - все операции, которые меняют `chat_messages`/variants/selection и/или создают “канонические” сообщения истории, проходят через эту очередь,
  - это защищает UI от гонок и делает порядок сообщений предсказуемым.

Практическое ограничение v1 (для простоты UX):

- в рамках одной ветки в один момент времени стримит в `chat_history` **только один** “main LLM” (canonical generation);
  остальные параллельные вычисления должны либо:
  - писать в артефакты (`panel:*`, `feed:*`, `internal`),
  - либо готовить variants, которые будут применены/выбраны через очередь коммитов.

Зависимости:

- зависимый шаг/пайплайн должен выражать зависимости **через артефакты**, а не через “ссылку на пайплайн”:
  - “жду, пока обновится `art.<tag>` (latest/last_n)”
  - “использую `art.<tag>.value` как вход”

Конфликты:

- если два независимых исполнителя пытаются одновременно **коммитить изменения истории**, порядок определяет очередь коммитов.
- если два исполнителя пытаются одновременно **записать один и тот же `tag` артефакта**, это должен быть:
  - либо один writer (остальные read-only),
  - либо конфликт/ошибка policy (v1 рекомендует single-writer per `tag`).

### Многошаговые LLM-вызовы внутри одного `PipelineRun` (aux вызовы пайплайна)

В v1 пайплайны могут выполнять **свои aux LLM-вызовы** (один или несколько) в рамках одного run, при условии, что:

- только **один** шаг является “main chat generation” (тот, который стримит и сохраняет assistant variant/message),
- остальные LLM шаги:
  - **не являются** частью “main llm” (не стримят в `chat_history`, не пишут `message_variants`),
  - производят артефакты/промежуточные результаты (например `Augmentation`),
  - по желанию могут быть использованы как входные данные для effective prompt **только через** механизм артефактов (`PipelineArtifact` + `promptInclusion`).
  
Это не меняет принцип работы “main llm”: каноническая генерация остаётся одной и реализована существующим chat-core.

Типовой паттерн (пример):

1. `pre` — собирает контекст (history, char, user, policy), но **не зовёт LLM**.
2. aux LLM шаг — зовёт LLM и генерирует `Augmentation` (план/заметки/CoT‑подобный скретчпад) как артефакт.
3. `llm` (main) — основной вызов LLM, который **стримит** и сохраняет assistant ответ; при этом `Augmentation` вставляется в PromptDraft **только для этой генерации**.

#### Как именно вставляется `Augmentation`

`Augmentation` добавляется в PromptDraft как синтетическое “prompt-only сообщение”:

- **место**: строго **после последнего user-сообщения** (перед main-генерацией),
- **роль**: настраиваемая (`assistant` по запросу пользователя, либо более безопасно `developer/system`), но в любом случае это **не запись в историю**,
- **видимость**: `prompt_only` (не UI, не history),
- **время жизни**: только на один `Generation` (main).

> Важно: это сообщение не создаёт `chat_messages` и не влияет на варианты/ветки; оно существует только внутри PromptDraft (и опционально в redacted prompt snapshot).

### Контекст исполнения (PipelineRunContext)

Каждый шаг видит:

- **Идентификаторы**:
  - `runId`, `stepRunId`
  - `chatId`, `branchId`, `entityProfileId`
  - `userMessageId?`
  - `assistantMessageId`, `assistantVariantId`
  - `generationId?`
- **Данные** (read-only источник правды):
  - `entityProfile.spec` (CharSpec)
  - `selected user person`
  - history (как `promptText` выбранного состояния)
- **Runtime**:
  - выбранный LLM runtime (provider/model/token)
  - LLM settings для текущего вызова (v1: **read-only passthrough** из запроса пользователя; пайплайны их не меняют)
- **Abort**:
  - `AbortSignal` (единый для run/step)
- **Policy**:
  - разрешения на типы мутаций (см. ниже)
  - лимиты контекста (limit N сообщений, max tokens и пр.)

### Session (artifact session) в исполнении (v1)

В рантайме шаги работают с “материализованным” представлением persisted-артефактов чата — `SessionView`:

- `SessionView` — это словарь `tag -> { value, history?, meta }`, который используется:
  - для Liquid-доступа `art.<tag>`,
  - как вход для шагов, читающих `PipelineState`/другие артефакты.

Упрощение v1:

- сессия **только chat-scoped** (ветки пока игнорируем); `branchId` остаётся в контексте, но не влияет на storage артефактов.

Загрузка данных:

- в v1 **не вводим** отдельный декларативный deps-контракт. `SessionView` строится из persisted-артефактов текущей сессии (чата) и используется шагами по факту необходимости.

## Определение пайплайна (Pipeline Definition)

В v1 достаточно линейного описания:

- список шагов по порядку,
- тип шага (`pre|rag|llm|post|tool`),
- параметры шага (json),
- условие включения (опционально).

> Замечание: шаги — это не обязательно “плагины”. В v1 шаги могут быть реализованы как встроенные режимы оркестратора + конфиг.



========================================
FILE: 20-mutation-policy.md
========================================

# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Mutation policy

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

## Модель влияния: два контура мутаций

Чтобы пайплайны были и мощными, и объяснимыми, вводим разделение:

### A) Prompt-time (без изменения БД истории)

Шаги могут:

- выбирать/триммить сообщения для prompt,
- преобразовывать контент для prompt (нормализация, “свернуть” старое в summary **только в prompt**),
- добавлять system/developer инструкции,
- (v2+) добавлять RAG/tool результаты как prompt-time инъекции,
- добавлять prompt-only инъекции (augmentation и т.п.) для улучшения качества **без изменения** канонической истории.

Но при этом:

- исходные `chat_messages`/`message_variants` в БД не меняются.

### B) Canonical-state (сохранение результата текущего хода)

Шаги могут:

- формировать итоговый `message_variants.promptText`,
- формировать итоговый `message_variants.blocksJson` (опционально),
- проставлять метаданные (citations, safety flags, structured output).

### C) Исторические трансформации (не по умолчанию)

Отдельный класс операций, которые действительно изменяют историю (например: заменить пачку старых сообщений summary-эквивалентом).

Рекомендация v1:

- либо **не включать**,
- либо реализовывать как добавление нового “summary message” без удаления старого,
- либо делать это отдельным, явно вызываемым action/endpoint (не часть обычной отправки сообщения).

## Политика мутаций (Mutation Policy)

### Зачем

Без политики мутаций пайплайн может стать “магией”, которая:

- ломает воспроизводимость prompt,
- непредсказуемо меняет историю,
- делает UI/варианты несогласованными.

### Базовое правило v1

По умолчанию разрешено:

- Prompt-time мутации PromptDraft,
- Canonical-state мутации текущего assistant variant/message,
- Логирование (step input/output, generation params/snapshot).

Запрещено по умолчанию:

- Мутации сообщений, не относящихся к текущему ходу:
  - по умолчанию нельзя менять прошлые `chat_messages`/variants
  - в рамках текущего хода допускается `message_transform` только для **текущего `userMessageId`**, если явно разрешено policy
- Удаление/перезапись history,
- Изменение selected variant “задним числом” у прошлых сообщений.

### Разрешённые “write targets” (v1)

Пайплайн может писать только в:

- `pipeline_runs`, `pipeline_step_runs` (лог процесса)
- `llm_generations` (статус, params, snapshot, usage)
- **сообщения текущего хода (turn)**:
  - текущие `message_variants` (assistantVariantId)
  - текущий `chat_messages` (assistantMessageId) — как кэш выбранного варианта
  - текущий `chat_messages` (userMessageId) и его variants — **только для текущего входа**, если шагу разрешено `message_transform`
- **`pipeline_artifacts`** (см. ниже) — если шагу разрешено `artifact_write` / `state_write`

Дополнение (важно для безопасности и детерминизма v1):

- **Single-writer per persisted `tag`**: шаг может писать только в те persisted-артефакты `art.<tag>`, которые принадлежат его пайплайну (writer-ownership).
- Попытка записать в чужой `tag` должна приводить к **ошибке policy** (runtime guard), даже если конфиг был провалидирован при сохранении `PipelineProfile`.

Запрещено по умолчанию (остается как guardrail v1):

- изменять **прошлые** `chat_messages`/variants (не относящиеся к текущему ходу) — только через отдельную, явно оформленную “историческую трансформацию”.



========================================
FILE: 30-step-types.md
========================================

# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Step types

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

## Step Types (семантика)

### `pre`

Назначение:

- собрать `systemPrompt` (Liquid template + policy composition),
- выбрать/подготовить history для prompt,
- нормализовать входы (user message, char/persona fields),
- подготовить `PromptDraft`.

Выход:

- `PromptDraft` готов к отправке в LLM.
- `pipeline_step_runs.outputJson` содержит: выбранный templateId/имя, системный промпт, параметры тримминга, статистику (кол-во сообщений), возможные предупреждения.

Дополнение (v1): `message_transform` для текущего user input

Иногда пользователи захотят “переписать” user-сообщение (пример: список действий → литературный текст) так, чтобы:

- в UI отображалась “красивая” версия,
- в prompt уходила “красивая” версия,
- но оригинал был доступен для дебага/отката.

Это решается через **variants для user-сообщений**, а не через разрушительное редактирование:

- шаг с правом `message_transform` может:
  - создать variant для **текущего** `userMessageId` (например `kind=normalized|rewritten|dice_enriched`)
  - сделать его selected/active (policy-controlled)
  - сохранить исходный ввод как отдельный variant (например `kind=raw_user_input`) или оставить как “исходный selected” и переключить selection

Таким образом инвариант `promptText` сохраняется: мы всё так же берём `promptText` **выбранного** состояния, просто “выбранным” может стать нормализованный вариант.

Не-цель v1:

- массово переписывать прошлые user-сообщения “задним числом” без отдельной операции (см. исторические трансформации).

### `rag` (опционально)

Назначение:

- найти дополнительные знания/контекст,
- добавить их в PromptDraft так, чтобы это было наблюдаемо и воспроизводимо.

Статус реализации:

- **v1**: не является целью реализации в текущем срезе. Раздел оставлен как “место для контракта”.
- **v2+**: ожидается отдельная, более подробная спека под RAG (retrieval, источники, формат цитат, лимиты, безопасность).

Рекомендация формы инъекции (v1):

- либо добавить один “system appendix” блок в system prompt,
- либо добавить отдельное “developer note” сообщение в PromptDraft (не в БД историю).

Выход:

- `PromptDraft` дополнен `rag` результатами.
- В `pipeline_step_runs.outputJson`: список документов/чанков/цитат/скорингов (в пределах разумного).

### `llm`

Назначение:

- создать `Generation`,
- вызвать провайдера LLM со `PromptDraft.messages`,
- стримить delta наружу,
- делать throttled flush текста ассистента в БД.

Уточнение v1 (важно для совместимости):

- `llm` шаг рассматривается как **данность/граница существующего chat-core генератора**: текущий механизм стрима, flush в БД и финализации считается “уже работающим” и не переписывается ради v1 пайплайнов.
- Поэтому в v1 пайплайны не “управляют LLM” (не выбирают провайдера/модель/стрим-протокол внутри шага), а выступают как **обвязка до/после** этой точки:
  - `pre` — готовит контекст перед вызовом (например `systemPrompt`),
  - `llm` — вызывает существующий генератор,
  - `post` — обрабатывает результат после `done`.
- Единственный контролируемый вход на этой границе в v1 — **passthrough `settings`** из запроса пользователя (как opaque json), совместимый с текущей реализацией.

События (совместимо с текущим SSE):

- `llm.stream.meta`
- `llm.stream.delta`
- `llm.stream.error`
- `llm.stream.done`

Логирование:

- `llm_generations.paramsJson` — LLM settings.
- `llm_generations.promptSnapshotJson` (опционально) — финальный PromptDraft (или “почти финальный”).
- `llm_generations.promptTokens/completionTokens` (опционально) — usage.

Дополнение (v1): `llm` шаги бывают двух видов по эффекту:

- **aux LLM (prompt-time)**: создаёт `Augmentation` или другие артефакты для PromptDraft; не стримит в UI и не пишет в `message_variants`.
- **main LLM (canonical)**: единственный шаг, который стримит и сохраняет assistant variant/message.

Примечание v1:

- aux LLM вызовы пайплайна не являются частью “main llm” и не меняют его механику; они могут только производить артефакты, которые затем включаются в effective prompt через `promptInclusion`.

### `post` (опционально, но рекомендуется)

Назначение:

- финально обработать результат генерации,
- привести к каноническому виду для хранения/рендера,
- (опционально) сформировать `blocksJson`.

Примеры задач:

- нормализация markdown/пробелов,
- safety redaction,
- парсинг структурированного вывода (JSON-mode),
- разбор на blocks (answer/tool_result/reasoning(ui_only)).

Важно:

- В v1 рекомендуется делать `post` **после** `done`, а не в потоковом режиме.

Дополнение: `post` может быть реализован как “отдельный runner” на другой модели (как LLM-call внутри post), но его **write targets** по умолчанию ограничены:

- обновление текущего assistant message/variant (format/safety/blocks),
- обновление `pipeline_artifacts` (обычно `kind=state`, если разрешено `state_write`),
- без изменения прошлых `chat_messages`.

### `tool` (опционально)

Назначение:

- выполнить внешнюю функцию/интеграцию (HTTP, поиск, парсер) как шаг пайплайна.

Статус реализации:

- **v1**: не является целью реализации в текущем срезе (в проекте пока нет полноценного механизма tool/RAG). Раздел оставлен как “контур”.
- **v2+**: ожидается отдельная спека под tool execution (sandboxing/таймауты/ретраи/форматы результатов/политики ошибок).

В потенциальной модели (v2+) это может существовать как “tool-before-llm” (до LLM) или “tool-only” (без LLM), но без сложного loop-а.



========================================
FILE: 40-artifacts.md
========================================

# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Artifacts

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

## PipelineArtifact, блоки и “внешние” UI поверхности

Этот раздел фиксирует расширение модели: вместо того чтобы ограничиваться `blocksJson` “только в сообщениях”,
мы вводим **PipelineArtifact** как универсальную сущность, которая покрывает:

- prompt-only вставки (pre-CoT augmentation),
- ui-only контент (комментарии/индикаторы/спойлеры),
- prompt+ui данные,
- “внешние” UI поверхности: панели/ленты/оверлеи, которые не являются сообщениями истории.

### Две независимые оси: `visibility` и `uiSurface`

Ключевая идея: “участвует ли в prompt” и “где отображается” — разные вещи.

- **`ArtifactVisibility`** определяет участие в prompt/UI (`prompt_only | ui_only | prompt_and_ui | internal`).
- **`UiSurface`** определяет где показывать в UI (`chat_history | panel:* | feed:* | overlay:*`).

Примеры:

- Pre-CoT: `visibility=prompt_only`, `uiSurface=internal` (или вообще без UI surface), `retention=ephemeral`.
- RPG state: `visibility=prompt_and_ui`, `uiSurface=panel:rpg_state`, `retention=durable`.
- “Реддит-комментаторы”: `visibility=ui_only`, `uiSurface=feed:commentary`, `retention=durable|ttl`.
- “GM note to player, и чтобы LLM тоже видел”: `visibility=prompt_and_ui`, `uiSurface=chat_history` (или `panel:*`), `retention=durable`.

### Базовая позиция v1

- Канон для prompt — `promptText`.
- `PipelineArtifact` — каноническая единица “созданного пайплайном контента”.
- В v1 **композиция влияний нескольких пайплайнов на effective prompt** выполняется **только через артефакты** (`PipelineArtifact` + `promptInclusion`).  
  Прямые конкурентные правки одного `PromptDraft` несколькими пайплайнами в v1 не формализуем (v2+).
- `blocksJson` остаётся, но трактуется как:
  - либо denorm представление части артефактов для `uiSurface=chat_history`,
  - либо legacy-совместимый UI-слой (пока не мигрировали UI на чтение артефактов напрямую).

### Артефакты в prompt: правила включения (prompt injection)

Чтобы prompt был воспроизводим и объясним, для артефактов задаётся явная политика включения:

- **`promptInclusion.mode`**:
  - `none` (никогда не включать в prompt)
  - `append_after_last_user` (после последнего user сообщения) — ключевой кейс pre-CoT
  - `prepend_system` (в начале system)
  - `as_message` (как отдельное синтетическое сообщение с ролью)
- **`promptInclusion.role`**: `system | developer | assistant | user` (по умолчанию безопаснее `developer`/`system`)
- **`promptInclusion.format`**: `text | json | markdown` (и правила сериализации)

`promptInclusion.mode` — это **shorthand** для самых частых кейсов.
При необходимости он раскрывается в более детальные поля (`operation/target/anchor/phase/priority`), описанные ниже.

> Важный принцип: артефакт с `uiSurface=panel:*` может участвовать в prompt, если `visibility` допускает и задан `promptInclusion`.

#### Роли: доменная роль vs роль провайдера

Чтобы не привязываться к конкретным API провайдера, различаем:

- **доменную роль**: `system | developer | user | assistant`
- **роль провайдера** (то, что реально уходит в LLM API): обычно `system | user | assistant`

Правило v1:

- `developer` маппится в `system` (или в отдельный “system section”), пока не введём отдельную provider-роль везде.

Практические эвристики:

- артефакт, созданный автоматикой/LLM (pre/post), по умолчанию **не должен** идти как `user`,
- `user`-роль допустима, если артефакт является “проекцией/нормализацией” **текущего user ввода** (см. `message_transform` ниже).

#### Операции включения: `insert / replace / wrap / merge`

Одного `mode` недостаточно: пользователи захотят “replace/merge/append” и вставки “на глубину”.
Поэтому концептуально вводим:

- **`promptInclusion.operation`** (что делаем с target):
  - `insert` — вставить синтетическое prompt-only сообщение
  - `replace` — заменить target (system или конкретное сообщение) **в PromptDraft**
  - `wrap` — обернуть target: `pre` + target + `post`
  - `merge_section` — обновить/слить именованную секцию (в system или в JSON state)
- **`promptInclusion.target`**:
  - `system` (system prompt целиком или его секция)
  - `timeline` (вставка как сообщение в PromptDraft.messages)
  - `message_ref` (конкретное сообщение, выбранное селектором/якорем)

> Важно: операции `replace/wrap/merge_section`, применённые к сообщениям, по умолчанию действуют **только на PromptDraft**.
> Каноническая история меняется только через `message_transform` (см. ниже) или через отдельные “исторические трансформации”.

#### Якоря/вставка “на глубину” (anchor + depth policy)

Чтобы можно было вставить “на 4 сообщения назад” и при этом не зависеть от случайного trimming,
позиционирование задаётся как **селектор + политика**:

- **`promptInclusion.anchor`** (селектор позиции):
  - `after_last_user`
  - `before_last_assistant`
  - `after_message_id:<id>`
  - `relative_to_end(offset=-4, roleFilter=any|user|assistant)`
  - `relative_to_anchor(anchor=after_last_user, offset=-4)` (если хочется “от последнего user”)
- **`promptInclusion.place`**: `before | after | replace | wrap`
- **`promptInclusion.depthPolicy`** (если якорь/сообщение не попало в окно контекста из-за trimming):
  - `strict_drop` — не вставлять
  - `clamp_to_oldest_kept` — вставить рядом с самым старым сообщением, которое осталось
  - `relocate_to_nearest` — переякорить к ближайшему доступному

Это покрывает кейс “лоровые заметки поближе к актуальному контексту, но не в самом конце”.

#### Порядок (ordering) артефактов в prompt

Если несколько артефактов включаются в prompt, порядок должен быть детерминированным.

Вводим поля:

- **`promptInclusion.phase`** (грубая стадия): `system | early | near_anchor | after_last_user | tail`
- **`promptInclusion.priority`** (число, чем меньше — тем раньше внутри phase)

Детерминированный tie-break v1 (если совпали):

1. порядок пайплайна в `PipelineProfile` (выше → раньше)
2. порядок шагов пайплайна (PipelineStepRun order)
3. `tag` (лексикографически; стабильный ключ)
4. `version` (если применимо) / `id`

Это позволяет пользователям контролировать “в каком порядке оно добавляется в prompt”.

Рекомендуемо в v1:

- Стримить только текст (`llm.stream.delta`), обновляя буфер `promptText`.
- Генерировать/обновлять `blocksJson` **только на финале** (в `post`).

Опционально (v2+):

- добавить SSE события для blocks:
  - `message.blocks.ready` (одним payload после done)
  - `message.block.delta` (инкрементальные блоки)

### Reasoning

Если reasoning хранится:

- хранить как block `type=reasoning` с `visibility=ui_only`,
- по умолчанию reasoning не должен попадать в `promptText`.

> В терминах артефактов reasoning — это либо `PipelineArtifact` с `visibility=internal/ui_only`,
> либо блок внутри `uiSurface=chat_history`-артефакта (в зависимости от того, как будет удобнее UI).

---

## PipelineArtifact storage & `art.<tag>` view (v1)

Этот раздел фиксирует отдельную сущность, которая позволяет “поломать” текущий минимализм ради удобного API и расширяемости.

Идея: pipeline step может создавать/обновлять артефакты разных типов, а UI и PromptDraft “подписываются” на них.

### Зачем отдельная сущность

`chat_messages` и `message_variants` хорошо подходят для истории диалога, но плохо масштабируются под:

- внешние UI поверхности (панели/ленты),
- prompt-only вставки,
- состояния/память, которая живёт “поверх” истории,
- комментирующие/мета-слои, не являющиеся частью истории.

### Базовая модель артефакта (логическая)

- `id`
- `ownerId`
- `sessionId` (v1: `chatId`) — “место жизни” persisted-артефактов (см. термин **Session** в `00-overview.md`)
- `tag`:
  - `access=persisted`: string, **обязательно**, уникально (единая точка адресации как `art.<tag>`)
  - `access=run_only`: опционально (может отсутствовать или быть локальным; уникальность не требуется)
- `kind` (string) — например: `augmentation | state | commentary | stats | any`
- `access` (`run_only | persisted`) — определяет, доступен ли артефакт как `art.<tag>` и может ли быть входом для других пайплайнов
- `visibility` (`prompt_only | ui_only | prompt_and_ui | internal`)
- `uiSurface` (`chat_history | panel:<id> | feed:<id> | overlay:<id> | internal`)
- `contentType` (`text | json | markdown`)
- `contentJson` / `contentText`
- `promptInclusionJson` (nullable) — правила включения в prompt (см. выше)
- `retentionPolicyJson` — overwrite/append/ttl/maxVersions и т.п.
- `version` (int или uuid)
- `basedOnVersion` (nullable)
- `createdAt`, `updatedAt`

Дополнение (важно для твоего кейса со “статами”):

- **Логический persisted-артефакт** определяется ключом \(ownerId, sessionId, tag\).  
  Остальные поля (`kind`, `uiSurface`, `visibility`) — свойства этого артефакта и не участвуют в уникальности имени.

Правило уникальности `tag` (v1):

- `tag` уникален **в пределах (`ownerId`, `sessionId`)** для `access=persisted` (v1: “в пределах чата”).
- попытка создать второй persisted-артефакт с тем же `tag` — **ошибка конфигурации/валидации** (коллизия имени).
- Поле `version` делает артефакт **версионируемым**: одна логическая сущность может иметь много версий (история изменений).
- Для UI и для prompt почти всегда нужен не “все версии”, а **материализованное представление** (обычно “последняя версия”).
  Это можно реализовать либо отдельным “head pointer”, либо запросом “latest by createdAt/version”.

### Доступ из LiquidJS

Останавливаемся на одном пространстве имён:

- **`art.<tag>`** — доступ к артефакту по уникальному `tag`.

Ключевая договорённость (shape):

- `art.<tag>.value` — **последний (актуальный) результат** артефакта (то, что “получили бы по tag.value”):
  - либо `contentJson`,
  - либо `contentText` (зависит от `contentType`).
- `art.<tag>.history[]` — массив **предыдущих** значений `value` *в том же формате*, размер/состав которого определяется политикой “жизни” (retention).
  - `history` **не включает** текущее `value`.

Уточнение:

- `art.<tag>` относится только к артефактам с `access=persisted`.
- `access=run_only` артефакты существуют как промежуточные результаты внутри run и (в v1) наблюдаемы через `pipeline_step_runs`/snapshot, но не как адресуемые `art.*`.

> Реальный синтаксис доступа/фильтров Liquid (например “последний элемент history”) уточним позже.
> Здесь фиксируем именно **семантику** и гарантии структуры.

### Правила write/read доступа (v1)

Ключевой guardrail (v1):

- **Single-writer per persisted `tag`**: каждый persisted-артефакт `art.<tag>` имеет **ровно одного writer-а** (пайплайн-владельца).  
  Пайплайн **не может** писать в `tag`, который ему не принадлежит.

Упрощение v1:

- persisted-артефакты в рамках **Session** считаются **доступными на чтение** для шагов/пайплайнов (границы задаются не ACL между пайплайнами, а:
  - тем, что включаем в prompt через `promptInclusion`,
  - тем, что показываем в UI через `visibility/uiSurface`,
  - и дисциплиной использования (пайплайн читает только то, что ему нужно).

Рекомендация реализации:

- уникальность `tag` и single-writer проверяются при сохранении/активации `PipelineProfile` (валидация “нет коллизий”),
- при выполнении шага дополнительно действует runtime-guard: попытка `artifact_write` в чужой `tag` → ошибка policy (защита данных).

### Модель хранения Session (v1): Latest + History

В v1 фиксируем “практическую” модель:

- сессия хранит **актуальное состояние** каждого `art.<tag>` (то, что выдаётся как `art.<tag>.value`);
- история (`art.<tag>.history[]`) хранится **только если** retention политика для этого `tag` её требует.

Концептуально это “Latest + History”:

- **Latest**: материализованное текущее значение `value` для `(ownerId, sessionId, tag)`
- **History**: набор версий (append-only или windowed), управляемый `retentionPolicyJson`

Это позволяет быстро строить `SessionView` и не превращать сессию в “один большой JSON” как источник правды.

### PipelineState как частный случай артефакта (`kind=state`)

**PipelineState** — это артефакт с:

- `kind=state`
- `contentType=json`
- `visibility=prompt_and_ui` (часто) или `ui_only` (если в prompt не надо)
- `uiSurface=panel:*` (часто), но может быть и `chat_history`

Семантика:

- хранит “состояние мира/сессии”: время, локация, активные персонажи, флаги и т.п.
- **не является сообщением** и не хранится в `chat_messages`

### Зависимость от прошлого выполнения

Шаг, обновляющий state (часто post-пайплайн), читает:

- последние \(N\) сообщений (например 5),
- текущий `PipelineState` по тегу (если существует): `art.<tag>.value`,
- при необходимости историю: `art.<tag>.history[]` (в пределах retention),
- при необходимости другие артефакты (например stats/feeds),
- и вычисляет новую версию состояния.

Критично: шаг должен знать, **какую версию** он использовал как базу (`basedOnVersion`), чтобы логировать “до/после”.

### Конфликты обновления persisted-артефактов (v1 минимум)

В v1 фиксируем самое простое и безопасное поведение: **optimistic concurrency + reject**.

- При записи новой версии persisted-артефакта `art.<tag>` шаг указывает `basedOnVersion` (версию, на базе которой вычислял результат).
- Оркестратор применяет запись **только если** текущая `latest` версия по `(ownerId, sessionId, tag)` совпадает с `basedOnVersion`.
- Если `basedOnVersion` не совпал (кто-то успел обновить `latest`, либо произошёл повтор/гонка) — запись отклоняется ошибкой (например `artifact_conflict`).

Auto-retry / merge / last-write-wins — сознательно откладываем на v2+.

### Retention / история артефактов (управляется пользователем)

Пользователь выбирает политику хранения per-tag/per-kind:

- хранить только “последнюю версию” (overwrite),
- хранить историю версий (append-only) в пределах:
  - лимита по количеству версий,
  - TTL по времени,
  - либо обоих ограничений.

В терминах “время жизни” (как ты описал) это удобно нормализовать до трёх режимов:

1. **Ephemeral (одноразовое)**:
   - существует только в рамках одного `PipelineRun` / одной `Generation`
   - типичный пример: pre-CoT `Augmentation`
   - хранение: не как полноценный “долговременный” артефакт, а:
     - либо только в `pipeline_step_runs.outputJson` (redacted/summary/hash),
     - либо в `llm_generations.promptSnapshotJson` (redacted), чтобы дебажить “что реально ушло в prompt”
2. **Windowed (окно по ходам/версиям)**:
   - хранить последние \(N\) версий / \(N\) ходов / TTL
   - в prompt можно включать “последние N результатов” (например последние 5)
   - всё, что старее окна, **не удаляем бесследно**: переносим в архив/снапшот (см. ниже)
3. **Durable (условно “вечно”)**:
   - хранить все версии (или очень большой лимит)
   - типичный пример: бесконечная/длинная лента “reddit-комментариев”

#### Архивирование “старого” (snapshot/архив)

Чтобы “всё старее окна попадает в снепшот” было формализовано, вводим правило:

- при “prune” версий (по window/ttl) система может сохранять:
  - агрегированный снапшот в `pipeline_step_runs.outputJson` (например “сводка старых комментариев”),
  - и/или ссылку на архивные записи (если мы заведём отдельную таблицу `pipeline_artifact_archives` в будущем),
  - и/или просто оставить след в логе шага (какие версии были удалены/сжаты).

> Важно: `llm_generations.promptSnapshotJson` логирует prompt конкретной генерации.  
> Для артефактов “длиннее одного хода” лучше иметь отдельный механизм архива, чтобы не привязывать архив к конкретной генерации.

### “Жизнь в UI” для артефактов, которые живут дольше одного хода

Проблема, которую ты описал (статы): UI не должен показывать 5 старых версий как “актуальные”, но пайплайн должен видеть историю.

Решение: разделить **данные** и **проекцию**.

- **Данные**: версионируемый `PipelineArtifact` (append-only или overwrite+history).
- **UI-проекция** (по умолчанию):
  - для `uiSurface=panel:*` (например `panel:rpg_state`) UI показывает **только последнюю версию** (materialized view),
  - для `uiSurface=feed:*` UI показывает **ленту версий** (timeline),
  - для `uiSurface=chat_history` — либо “вставка” как отдельный элемент истории (если `prompt_and_ui`), либо как UI-only блок.

Это можно описать через поле (логически) `uiPresentationJson`, например:

- `uiPresentation.mode`:
  - `latest_only` (панели/статы)
  - `timeline` (ленты)
  - `diff` (панель “что изменилось”)
- `uiPresentation.maxItems` (для лент)

### “Жизнь в prompt” для артефактов, которые живут дольше одного хода

Аналогично UI, prompt почти никогда не должен включать “всю историю state”.

Рекомендуемая модель выбора версий для prompt:

- `promptInclusion.versionSelector`:
  - `latest` — включать только `art.<tag>.value` (идеально для `kind=state`)
  - `last_n` — включать `art.<tag>.value` + последние \(N-1\) элементов из `art.<tag>.history[]`
  - `all` — включать всё (обычно опасно по токенам; только для небольших артефактов)

Итого, для статов:

- **UI**: `uiPresentation.mode=latest_only`
- **Prompt**: `versionSelector=latest`
- **Pipeline логика**: при обновлении читает `latest` + при необходимости `last_n` (история для вычисления дельт)

### Write policy: артефакты как отдельный write target

Чтобы не ломать инвариант “не переписываем историю чата”, `pipeline_artifacts` является **отдельным write target** с отдельным разрешением:

- `artifact_write` — создание/обновление любых артефактов
- `state_write` — создание/обновление только `kind=state` (более узкое и безопасное)

### Логирование state/artifact update

Рекомендуется писать в `pipeline_step_runs`:

- какой артефакт читали/писали (`tag`, `kind`, `scope`),
- какую версию брали как базу (`basedOnVersion`),
- какая новая версия получилась,
- diff/сводку изменений (не обязательно весь state дублем, если он большой).



========================================
FILE: 50-observability.md
========================================

# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Observability

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

## Наблюдаемость и воспроизводимость

Подробные правила логирования вынесены отдельно, чтобы не раздувать остальные разделы спеки: см. `55-logging-and-reproducibility.md`.

### Что важно логировать (v1 минимум)

- `pipeline_runs`:
  - trigger, status, started/finished, привязки к chat/entityProfile
- `pipeline_step_runs`:
  - stepType, статус, длительность
  - inputJson/outputJson (в разумных пределах)
- `llm_generations`:
  - provider/model, paramsJson, status
  - (опционально) promptSnapshotJson, usage tokens

Дополнения (redaction, augmentation, `promptHash`, лимиты/retention) — см. `55-logging-and-reproducibility.md`.



========================================
FILE: 55-logging-and-reproducibility.md
========================================

# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Logging & reproducibility

> Статус: **draft** для обсуждения.  
> Цель: вынести правила логирования/наблюдаемости в отдельный документ, чтобы не “мусорить” ими в описании шагов/артефактов/исполнения.

## Зачем (v1)

Логирование должно отвечать на вопросы:

- **Почему получился такой ответ?**
- **Что именно ушло в LLM (в redacted виде)?**
- **Какие шаги выполнялись и что они сделали?**
- **Где случилась ошибка/аборт и на каком шаге?**

## User-facing debug report (“как A,B,C → X”)

В v1 считаем нормой, что пользователь может открыть “debug report” и получить **понятную картинку**, что именно “работало под капотом” для формирования результата.

Минимальный состав такого отчёта:

- **Почему запустилось**:
  - `trigger` (новое сообщение / regenerate / manual / api)
  - ключ дедупликации turn-а (чтобы было видно “это повтор или новый запуск”)
- **Что было входом**:
  - user message (id + текст выбранного варианта)
  - полная история чата (как канон, `promptText` выбранных variants)
- **Что реально ушло в LLM (effective prompt)**:
  - финальный `PromptDraft.messages[]` в порядке отправки (role + content), в redacted виде при необходимости
  - маппинг ролей (например `developer` → `system`), если применялся
  - сведения о trimming: что было исключено и почему (лимиты/правила/сводка)
- **Какие “инструменты” участвовали**:
  - активный `PipelineProfile` (id/версия) и список пайплайнов/шагов с их статусами/таймингами
  - для каждого шага — кратко “что изменил/добавил” (без больших полотен текста)
- **Какие артефакты использовались**:
  - список `art.<tag>`, которые были прочитаны/включены в промпт (tag + version + как использовалось)
  - список `art.<tag>`, которые были обновлены (old/new version, `basedOnVersion`, краткий diff/summary)
- **Параметры генерации**:
  - provider/model, `paramsJson`, usage tokens (если есть), длительность
- **Ограничения приватности**:
  - явная отметка, что reasoning/working-notes либо отсутствуют, либо представлены как redacted/summary/hash.

## Принципы

- **Backend source of truth**: каноническая “истина процесса” хранится в БД (`pipeline_runs`, `pipeline_step_runs`, `llm_generations`).
- **Минимум, но достаточно**: логируем то, что помогает объяснить результат и дебажить, но не превращаем БД в “сырые логи”.
- **Redaction-first**: всё, что потенциально содержит секреты/PII/“сырой reasoning”, хранится только в redacted/summary/hash форме.
- **Bounded size**: все поля логов имеют лимиты размера (truncate/omit), чтобы логирование не ломало производительность.

## Уточнение: что именно понимаем под “reproducibility” (v1 vs v2+)

Под “воспроизводимостью” здесь имеется в виду не то, что LLM всегда вернёт один и тот же ответ, а то, что мы можем:

- восстановить **effective prompt** (какие сообщения/инъекции реально ушли в провайдера),
- объяснить, **почему** в prompt попали именно эти части (trimming, policy, артефакты),
- сравнивать “одинаковые” промпты между запусками.

Практически это упирается в два артефакта наблюдаемости:

- `promptSnapshotJson` (redacted) — человеко-читаемая реконструкция.
- `promptHash` — машинный идентификатор “эквивалентного” промпта.

Ограничение v1:

- каноническую сериализацию `PromptDraft` для расчёта `promptHash` (нормализация, порядок ключей, правила эквивалентности) можно зафиксировать **на этапе реализации**, т.к. она зависит от конкретного представления `PromptDraft` и выбранного провайдера.

v2+ (если потребуется строгая воспроизводимость):

- вводим формальные правила canonicalization и “equivalence classes” для promptHash,
- вводим более строгие механизмы replay (в связке с SSE/Last-Event-ID и/или requestId).

## Корреляция (что связывает записи)

Минимальный набор идентификаторов, которые должны коррелировать записи между таблицами и SSE:

- `chatId`
- `userMessageId?`
- `assistantMessageId`, `assistantVariantId`
- `runId`
- `stepRunId`
- `generationId?`

## Что логируем (v1 минимум)

### `pipeline_runs`

- `trigger` (`user_message|regenerate|manual|api`)
- `status` (`running|done|aborted|error`)
- `startedAt/finishedAt`
- привязки к `chatId`/`entityProfileId`
- (рекомендуемо) ссылка/идентификатор активного `PipelineProfile` (и/или его ревизии), чтобы понимать “какой набор пайплайнов был включён”

### `pipeline_step_runs`

Общее:

- `stepType`
- `status` + `timings`
- `inputJson` / `outputJson` — **в разумных пределах** (см. лимиты ниже)
- `error` (code/message/details) при `error`

Рекомендация по содержимому `inputJson/outputJson`:

- Логировать **факты принятия решений** (например: выбранный template, параметры trimming, предупреждения, ids использованных сущностей), а не большие “полотна текста”.

### `llm_generations`

- `provider/model`
- `paramsJson` (settings)
- `status` (`streaming|done|aborted|error`)
- usage (`promptTokens/completionTokens`), если доступно
- (опционально) `promptSnapshotJson` — **только redacted** (см. ниже)
- (рекомендуемо) `promptHash` — стабильный хеш, чтобы сопоставлять “одинаковые промпты” без хранения сырых данных

## Политика redaction (v1)

### Что не храним “как есть”

- секреты/токены/ключи провайдеров
- персональные данные (если они могут быть в тексте)
- “сырой chain-of-thought / reasoning” модели

### `Augmentation` (pre-CoT / working-notes)

Разрешённые режимы хранения в логах:

- **redacted** — обрезка/маскирование
- **summary** — короткий synopsis (без деталей/секретов)
- **hash-only** — только хеш (если содержимое хранить нельзя)

Где хранить:

- `pipeline_step_runs.outputJson` (предпочтительно)
- и/или `llm_generations.promptSnapshotJson` (если augmentation реально был включён в prompt и это важно для дебага)

### `promptSnapshotJson`

- хранится только в redacted виде:
  - без секретов,
  - с урезанием размера,
  - с `promptHash`.

## Лимиты и retention (v1)

В v1 достаточно зафиксировать принципы (конкретные числа можно уточнить при реализации):

- `pipeline_step_runs.inputJson/outputJson`: truncate/omit больших полей, хранить только summary/ids.
- `promptSnapshotJson`: хранить редкий/ограниченный по размеру снапшот; остальное — через `promptHash`.
- большие артефакты/истории — не дублировать в логах целиком (достаточно ссылок: `tag`, `basedOnVersion`, `newVersion`, краткий diff/summary).

## Debug-only снапшоты (v1)

Если нужен “большой снапшот” для дебага, он должен быть:

- **debug-only** (не влияет на канон и не участвует в обычном флоу),
- храниться отдельно от основных сущностей процесса,
- иметь явные лимиты/TTL.

## Ошибки и аборты (логирование)

Минимум:

- кто упал: `runId/stepRunId/generationId`
- причина: `errorCode` + короткое `message`
- статусные поля (`aborted|error`)

Подробности (stack traces / raw payloads) — по возможности в серверных логах, а в БД — только safe summary.



========================================
FILE: 60-sse-events.md
========================================

# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — SSE events

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

## SSE / транспортные события

### Зачем (UI “не зависло”)

Помимо стрима текста ассистента, UI должен уметь показывать “что происходит”:

- какой пайплайн/запуск сейчас выполняется,
- на каком шаге он находится,
- чем завершилось (done/aborted/error),
- чтобы пользователь не думал, что система зависла.

### Минимум v1 (совместимо с текущим)

- `llm.stream.meta` — канонические id (chat/message/variant/generation/run)
- `llm.stream.delta` — `{ content: string }`
- `llm.stream.error` — `{ message, code?, details? }`
- `llm.stream.done` — `{ status: done|aborted|error }`

### Event envelope (минимум v1)

Чтобы UI мог корректно коррелировать события и показывать прогресс/тосты, каждое событие (кроме keep-alive) должно включать:

- `chatId`
- `runId`
- `pipelineId` + “человекочитаемое” имя/тег пайплайна (`pipelineName` или `pipelineTag`)
- `trigger` (если применимо)
- `stepRunId?`, `stepType?`
- `generationId?`, `assistantMessageId?`, `assistantVariantId?`, `userMessageId?` (по ситуации)
- `status?` (для событий завершения)

> В v1 допускается не вводить `seq`/replay; порядок обеспечивается порядком доставки в одном SSE соединении.

### Pipeline progress events (v1)

Эти события предназначены в первую очередь для UI (тосты/индикаторы/таймлайн):

- `pipeline.run.started` — запуск пайплайна начался
- `pipeline.run.done` — запуск завершён успешно (`status=done`)
- `pipeline.run.aborted` — запуск прерван пользователем/abort (`status=aborted`)
- `pipeline.run.error` — запуск завершился ошибкой (`status=error`, `error{code,message,details?}`)

Рекомендация:

- `pipeline.run.started` должно приходить как можно раньше (сразу после создания `PipelineRun`), чтобы UI мог показать “Pipeline <name>: в процессе”.
- одно из `pipeline.run.(done|aborted|error)` должно приходить ровно один раз.

### Step progress events (v1)

Если нужно в UI отображать прогресс внутри пайплайна:

- `pipeline.step.started`
- `pipeline.step.done`

Для шага полезно включать:

- `stepType`
- `stepRunId`
- `status` (для `done`: `done|aborted|error`) + `error?`
- (опционально) `label`/`summary` — короткая строка для UI (“Context: preparing…”, “Post: formatting…”; RAG/tool — v2+)

### Keep-alive (v1)

SSE соединения часто обрываются прокси/браузером без трафика. Рекомендуется периодически слать keep-alive:

- SSE comment (например `: keep-alive\n\n`) или отдельное событие `ping`.

### Обрыв соединения и “resume” (v1 минимум)

В v1 не требуем replay/`Last-Event-ID`.

Если соединение оборвалось:

- UI должен уметь **перечитать** состояние из БД через обычный API (variant текст + статусы `pipeline_runs/llm_generations`)
- и восстановить индикаторы “идёт/завершено/ошибка” по статусам.

Replay/`Last-Event-ID` и детерминированное восстановление потока событий — v2+.



========================================
FILE: 90-open-questions.md
========================================

# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Future & open questions

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

## Версии и расширения (на будущее)

### v2+: DAG и многошаговая агентика

Возможные расширения:

- шаги как граф зависимостей (parallel execution),
- tool↔LLM loops,
- conditional branching (if/else) внутри pipeline definition,
- “memory” сущности (summary/notes) как отдельный тип сообщений или отдельная таблица.
- разграничение доступа (ACL) к persisted-артефактам **между пайплайнами** (если появятся плагины/мультиюзер/недоверенный код).
- “настоящая” идемпотентность API (например `Idempotency-Key`/`requestId`) и политика безопасных авто‑ретраев.

### v2+: Исторические трансформации

Если появится “compaction/summarize history”, то:

- оформляется как отдельный stepType (например `history_compaction`),
- имеет строгую политику: что меняется, где хранится “до/после”,
- и обязательно логируется как отдельный action/run.

## Открытые вопросы (для согласования перед API)

### Уже договорились (фиксируем для v1)

- **PipelineProfile** — сохраняемый набор активных пайплайнов (конфиг), который:
  - может быть выбран глобально,
  - может быть закреплён как override для `entityProfile` и/или конкретного `chat`.
- **Session** для persisted-артефактов в v1 — **chat-scoped** (ветки откладываем ради упрощения реализации).
- **Tags**:
  - `art.<tag>` уникален в пределах (`ownerId`, `sessionId`=chatId),
  - один writer на persisted `tag` (пайплайн не пишет в чужой тег),
  - основная валидация коллизий — при сохранении/активации `PipelineProfile`,
  - runtime-guard на запись остаётся как защита данных.
- **Хранение артефактов**: модель **Latest + History** (текущее значение всегда доступно; история — по retention).
- **Session snapshot**: v1 — **debug-only** (возможный откат рассматриваем как v2+).
- **Deps-контракт пайплайна**: в v1 **не вводим** отдельный декларативный deps-контракт (это опциональная оптимизация реализации или тема v2+).
- **Конфликты обновления `art.<tag>`**: v1 минимум — optimistic concurrency с `basedOnVersion` и **reject** при mismatch (без auto-retry/merge).
- **Idempotency/ретраи**: v1 минимум — дедупликация `PipelineRun` по ключам turn-а (без авто‑ретраев; повторы через `regenerate`).

1. Делаем ли пайплайны строго “один ход” (user→assistant) в v1, или закладываем многошаговые сценарии сразу?
2. Где хранится pipeline selection: на уровне chat? entityProfile? global? (и как наследуется)
3. Хотим ли мы v1 “tool step” как часть pipeline (хотя бы до LLM), или откладываем?
4. Нужны ли UI-события по шагам (`pipeline.step.*`) уже в v1?
5. Нужен ли `promptSnapshotJson` в v1 (с redaction), или достаточно `pipeline_step_runs` логов?

### Оставшиеся вопросы (на следующую итерацию обсуждения)

6. Нужен ли v2+ `Idempotency-Key/requestId` на уровне API и какие авто‑ретраи допустимы без дублей/гонок?

