# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Execution model

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

> Примечание: это часть спеки, выделенная из монолита `pipelines-and-processing-spec-v0.1.md`.

## Pipeline execution model (v1)

### Триггеры запусков

Минимальный набор:

- `user_message`: пользователь отправил новое сообщение
- `regenerate`: регенерация варианта для assistant message
- `manual`: ручной запуск (кнопка/endpoint) — опционально
- `api`: внешний вызов API — опционально

### Высокоуровневый flow “одного хода”

1. Backend сохраняет user message (если trigger = `user_message`).
2. Backend создаёт assistant message + assistant variant (пустые) и создаёт `PipelineRun`.
3. Pipeline выполняет шаги (линейно): `pre` → (`rag`?) → `llm` → (`post`?).
4. `llm` шаг создаёт `Generation` и стримит delta события.
5. По завершению (done/abort/error) выполняется финализация:
   - финальный flush в БД,
   - `post` (если есть),
   - обновление статусов `pipeline_runs`, `pipeline_step_runs`, `llm_generations`.

### Параллельное исполнение и очередь коммитов (v1)

Мы допускаем, что часть пайплайнов/шагов может выполняться **параллельно** (например RAG, валидация, тулзы, расчёты),
но изменения, которые влияют на **историю чата и UI-поток сообщений**, должны применяться **детерминированно**.

Модель v1:

- **Execute (можно параллельно)**:
  - шаги/пайплайны выполняют вычисления,
  - создают/обновляют `PipelineArtifact` (особенно `internal`/`ui_only`/`prompt_only`),
  - готовят “предложения изменений” (patch/intent), но не коммитят их в историю чата.

- **Commit (строго последовательно)**:
  - для каждого `(chatId, branchId)` существует **очередь коммитов**,
  - все операции, которые меняют `chat_messages`/variants/selection и/или создают “канонические” сообщения истории, проходят через эту очередь,
  - это защищает UI от гонок и делает порядок сообщений предсказуемым.

Практическое ограничение v1 (для простоты UX):

- в рамках одной ветки в один момент времени стримит в `chat_history` **только один** “main LLM” (canonical generation);
  остальные параллельные вычисления должны либо:
  - писать в артефакты (`panel:*`, `feed:*`, `internal`),
  - либо готовить variants, которые будут применены/выбраны через очередь коммитов.

Зависимости:

- зависимый шаг/пайплайн должен выражать зависимости **через артефакты**, а не через “ссылку на пайплайн”:
  - “жду, пока обновится `art.<tag>` (latest/last_n)”
  - “использую `art.<tag>.value` как вход”

Конфликты:

- если два независимых исполнителя пытаются одновременно **коммитить изменения истории**, порядок определяет очередь коммитов.
- если два исполнителя пытаются одновременно **записать один и тот же `tag` артефакта**, это должен быть:
  - либо один writer (остальные read-only),
  - либо конфликт/ошибка policy (v1 рекомендует single-writer per `tag`).

### Многошаговые LLM-вызовы внутри одного `PipelineRun` (важно для “pre-CoT” кейса)

В v1 разрешается **несколько LLM шагов подряд** в одном run (линейно), при условии, что:

- только **один** шаг является “main chat generation” (тот, который стримит и сохраняет assistant variant/message),
- остальные LLM шаги производят **prompt-time артефакты** (например `Augmentation`) и не пишут в историю чата.

Типовой паттерн (пример):

1. `pre` — собирает контекст (history, char, user, policy), но **не зовёт LLM**.
2. `llm` (pre-llm / planner) — зовёт **другую** модель (дешевле/быстрее) и генерирует `Augmentation` (план/заметки/CoT-подобный скретчпад).
3. `llm` (main) — основной вызов LLM, который **стримит** и сохраняет assistant ответ; при этом `Augmentation` вставляется в PromptDraft **только для этой генерации**.

#### Как именно вставляется `Augmentation`

`Augmentation` добавляется в PromptDraft как синтетическое “prompt-only сообщение”:

- **место**: строго **после последнего user-сообщения** (перед main-генерацией),
- **роль**: настраиваемая (`assistant` по запросу пользователя, либо более безопасно `developer/system`), но в любом случае это **не запись в историю**,
- **видимость**: `prompt_only` (не UI, не history),
- **время жизни**: только на один `Generation` (main).

> Важно: это сообщение не создаёт `chat_messages` и не влияет на варианты/ветки; оно существует только внутри PromptDraft (и опционально в redacted prompt snapshot).

### Контекст исполнения (PipelineRunContext)

Каждый шаг видит:

- **Идентификаторы**:
  - `runId`, `stepRunId`
  - `chatId`, `branchId`, `entityProfileId`
  - `userMessageId?`
  - `assistantMessageId`, `assistantVariantId`
  - `generationId?`
- **Данные** (read-only источник правды):
  - `entityProfile.spec` (CharSpec)
  - `selected user person`
  - history (как `promptText` выбранного состояния)
- **Runtime**:
  - выбранный LLM runtime (provider/model/token)
  - LLM settings для текущего вызова
- **Abort**:
  - `AbortSignal` (единый для run/step)
- **Policy**:
  - разрешения на типы мутаций (см. ниже)
  - лимиты контекста (limit N сообщений, max tokens и пр.)

## Определение пайплайна (Pipeline Definition)

В v1 достаточно линейного описания:

- список шагов по порядку,
- тип шага (`pre|rag|llm|post|tool`),
- параметры шага (json),
- условие включения (опционально).

> Замечание: шаги — это не обязательно “плагины”. В v1 шаги могут быть реализованы как встроенные режимы оркестратора + конфиг.

