# TaleSpinner — Pipelines + Pre/Post Processing Spec (v0.1) — Execution model

> Статус: **draft** для обсуждения.  
> Цель: зафиксировать архитектуру пайплайнов и пре/постпроцессинга, совместимую с текущим chat-core флоу, чтобы дальше согласовать API и начать реализацию.

> Примечание: это часть спеки, выделенная из монолита `pipelines-and-processing-spec-v0.1.md`.

## Pipeline execution model (v1)

### Триггеры запусков

Минимальный набор:

- `user_message`: пользователь отправил новое сообщение
- `regenerate`: регенерация варианта для assistant message
- `manual`: ручной запуск (кнопка/endpoint) — опционально
- `api`: внешний вызов API — опционально

### Idempotency и ретраи (v1 минимум)

В v1 фиксируем “минимум, который защищает от дублей”, без сложной идемпотентности API:

- Для одного “turn” в рамках чата должен существовать **только один** канонический запуск пайплайна:
  - trigger `user_message`: ключ дедупликации = `(chatId, userMessageId)`
  - trigger `regenerate`: ключ дедупликации = `(chatId, assistantVariantId)` (или эквивалентный стабильный идентификатор “перегенерации”)
- Если сервер получает повторный запрос с тем же ключом, он **не создаёт** новый `PipelineRun/Generation`, а возвращает уже существующие идентификаторы и текущий статус.

Политика ретраев (v1):

- Автоматических ретраев пайплайна/LLM по умолчанию **нет**.
- Повтор попытки — это явный пользовательский `regenerate` (или новый `user_message`).

### Высокоуровневый flow “одного хода”

1. Backend сохраняет user message (если trigger = `user_message`).
2. Backend создаёт assistant message + assistant variant (пустые) и создаёт `PipelineRun`.
3. Pipeline выполняет шаги (линейно): `pre` → (`rag`?) → `llm` → (`post`?).
4. `llm` шаг создаёт `Generation` и стримит delta события.
5. По завершению (done/abort/error) выполняется финализация:
   - финальный flush в БД,
   - `post` (если есть),
   - обновление статусов `pipeline_runs`, `pipeline_step_runs`, `llm_generations`.

### Уточнение v1: `llm` как “boundary” существующей генерации

В v1 `llm` — это не “настраиваемый плагин”, а **точка вызова уже существующего механизма генерации** (chat-core):

- текущая реализация стрима SSE и периодического flush текста в БД считается “как есть”,
- пайплайн в v1 выполняет только подготовку **до** вызова (`pre`) и обработку **после** завершения (`post`),
- управление выбором провайдера/модели/протокола стрима внутри `llm` шага **не является целью v1** (мы сознательно не ломаем то, что уже работает).

Единственный допустимый “вход” в генерацию в v1 — passthrough `settings` из запроса пользователя (opaque json), совместимый с текущим генератором.

### Порядок пайплайнов и единственный `main llm` (уточнение v1)

В v1 фиксируем упрощение, важное для UX и детерминизма:

- **`main llm` строго один** “канонический” процесс стрима/сохранения на момент времени (в рамках ветки/чата).  
  Он зависит от template/сборки prompt и является единственным источником `llm.stream.*` для `chat_history`.
- Порядок запуска пайплайнов определяется **позицией в `PipelineProfile`**:
  - пайплайн “выше” выполняется раньше,
  - порядок шагов внутри пайплайна — как описано в definition (линейно).

Ограничение v1 (на момент реализации):

- кейсы, когда **несколько пайплайнов параллельно/совместно модифицируют один `PromptDraft`** (“композиция PromptDraft между пайплайнами”) сознательно не формализуем до итерации реализации (v2+/follow-up).

### Параллельное исполнение и очередь коммитов (v1)

Мы допускаем, что часть пайплайнов/шагов может выполняться **параллельно** (например валидация, расчёты; RAG/tool — v2+),
но изменения, которые влияют на **историю чата и UI-поток сообщений**, должны применяться **детерминированно**.

Модель v1:

- **Execute (можно параллельно)**:
  - шаги/пайплайны выполняют вычисления,
  - создают/обновляют `PipelineArtifact` (особенно `internal`/`ui_only`/`prompt_only`),
  - готовят “предложения изменений” (patch/intent), но не коммитят их в историю чата.

- **Commit (строго последовательно)**:
  - для каждого `(chatId, branchId)` существует **очередь коммитов**,
  - все операции, которые меняют `chat_messages`/variants/selection и/или создают “канонические” сообщения истории, проходят через эту очередь,
  - это защищает UI от гонок и делает порядок сообщений предсказуемым.

Практическое ограничение v1 (для простоты UX):

- в рамках одной ветки в один момент времени стримит в `chat_history` **только один** “main LLM” (canonical generation);
  остальные параллельные вычисления должны либо:
  - писать в артефакты (`panel:*`, `feed:*`, `internal`),
  - либо готовить variants, которые будут применены/выбраны через очередь коммитов.

Зависимости:

- зависимый шаг/пайплайн должен выражать зависимости **через артефакты**, а не через “ссылку на пайплайн”:
  - “жду, пока обновится `art.<tag>` (latest/last_n)”
  - “использую `art.<tag>.value` как вход”

Конфликты:

- если два независимых исполнителя пытаются одновременно **коммитить изменения истории**, порядок определяет очередь коммитов.
- если два исполнителя пытаются одновременно **записать один и тот же `tag` артефакта**, это должен быть:
  - либо один writer (остальные read-only),
  - либо конфликт/ошибка policy (v1 рекомендует single-writer per `tag`).

### Многошаговые LLM-вызовы внутри одного `PipelineRun` (важно для “pre-CoT” кейса)

В v1 разрешается **несколько LLM шагов подряд** в одном run (линейно), при условии, что:

- только **один** шаг является “main chat generation” (тот, который стримит и сохраняет assistant variant/message),
- остальные LLM шаги производят **prompt-time артефакты** (например `Augmentation`) и не пишут в историю чата.

Типовой паттерн (пример):

1. `pre` — собирает контекст (history, char, user, policy), но **не зовёт LLM**.
2. `llm` (pre-llm / planner) — зовёт **другую** модель (дешевле/быстрее) и генерирует `Augmentation` (план/заметки/CoT-подобный скретчпад).
3. `llm` (main) — основной вызов LLM, который **стримит** и сохраняет assistant ответ; при этом `Augmentation` вставляется в PromptDraft **только для этой генерации**.

#### Как именно вставляется `Augmentation`

`Augmentation` добавляется в PromptDraft как синтетическое “prompt-only сообщение”:

- **место**: строго **после последнего user-сообщения** (перед main-генерацией),
- **роль**: настраиваемая (`assistant` по запросу пользователя, либо более безопасно `developer/system`), но в любом случае это **не запись в историю**,
- **видимость**: `prompt_only` (не UI, не history),
- **время жизни**: только на один `Generation` (main).

> Важно: это сообщение не создаёт `chat_messages` и не влияет на варианты/ветки; оно существует только внутри PromptDraft (и опционально в redacted prompt snapshot).

### Контекст исполнения (PipelineRunContext)

Каждый шаг видит:

- **Идентификаторы**:
  - `runId`, `stepRunId`
  - `chatId`, `branchId`, `entityProfileId`
  - `userMessageId?`
  - `assistantMessageId`, `assistantVariantId`
  - `generationId?`
- **Данные** (read-only источник правды):
  - `entityProfile.spec` (CharSpec)
  - `selected user person`
  - history (как `promptText` выбранного состояния)
- **Runtime**:
  - выбранный LLM runtime (provider/model/token)
  - LLM settings для текущего вызова (v1: **read-only passthrough** из запроса пользователя; пайплайны их не меняют)
- **Abort**:
  - `AbortSignal` (единый для run/step)
- **Policy**:
  - разрешения на типы мутаций (см. ниже)
  - лимиты контекста (limit N сообщений, max tokens и пр.)

### Session (artifact session) в исполнении (v1)

В рантайме шаги работают с “материализованным” представлением persisted-артефактов чата — `SessionView`:

- `SessionView` — это словарь `tag -> { value, history?, meta }`, который используется:
  - для Liquid-доступа `art.<tag>`,
  - как вход для шагов, читающих `PipelineState`/другие артефакты.

Упрощение v1:

- сессия **только chat-scoped** (ветки пока игнорируем); `branchId` остаётся в контексте, но не влияет на storage артефактов.

Загрузка данных:

- в v1 **не вводим** отдельный декларативный deps-контракт. `SessionView` строится из persisted-артефактов текущей сессии (чата) и используется шагами по факту необходимости.

## Определение пайплайна (Pipeline Definition)

В v1 достаточно линейного описания:

- список шагов по порядку,
- тип шага (`pre|rag|llm|post|tool`),
- параметры шага (json),
- условие включения (опционально).

> Замечание: шаги — это не обязательно “плагины”. В v1 шаги могут быть реализованы как встроенные режимы оркестратора + конфиг.

