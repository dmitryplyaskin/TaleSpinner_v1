# v2 — Термины и определения (простым языком)

Этот документ задаёт **единый словарь** для спеки v2.
Если термин не определён здесь — считаем, что он ещё не согласован.

## 1) Главная идея v2 (в одном абзаце)

**Operation** в v2 — это **одна исполняемая операция** (одно действие).  
Она может что-то посчитать, сделать aux LLM‑вызов, прочитать/записать артефакты, подготовить изменения текущего хода и вернуть результат.  
Когда именно операция запускается, задаётся **точкой запуска (Hook)** — например “до main LLM”, “после main LLM”.  
А то, *почему вообще стартовал run*, задаётся **Trigger** — пока только `generate` и `regenerate`.

> Примечание: термин “Pipeline” (пайплайн) в v2 считаем **legacy** и стараемся не использовать, чтобы не путать “одно действие” с “конвейером из шагов”.

## 2) Сущности (что это такое)

### Operation (операция)

**Operation** — это **одно действие**: “сделай X”.

Примеры операций:

- LLM‑операция: делает свой запрос в LLM по своему промпту → выдаёт результат → применяет эффект (создать/выбрать variant, создать артефакт для prompt или UI и т.п.)
- RAG‑операция: ходит в RAG → возвращает данные (обычно как артефакт)
- Tool‑операция: выполняет функцию/интеграцию → возвращает результат (обычно как артефакт)

Важное: в v2 **операция не состоит из шагов**. Операция и есть “единица исполнения”.

Операция **ничего не говорит** про “до/после main LLM” сама по себе — это задаётся тем, **в каком hook** она стоит.

### Hook (точка запуска)

**Hook** — это “где внутри run запускаем операцию” (позиция относительно main LLM).
Пока фиксируем только как термин; конкретный список hook‑ов согласуем позже.

Примеры (черновые):

- `before_main_llm` — до основной генерации
- `after_main_llm` — после завершения основной генерации

### Trigger (триггер запуска)

**Trigger** — это “почему стартовал run” (какое действие пользователя/системы породило запуск).
Пока фиксируем минимальный набор:

- `generate` — обычная генерация (новый ответ на текущий ход)
- `regenerate` — перегенерация (повторная генерация вместо/как новый variant)

### OperationProfile (профиль операций)

**OperationProfile** — набор включённых операций и их конфигураций.
Это **конфиг**, а не выполнение.

Важно: профиль отвечает на вопрос “какие операции запускать в каком hook”.
При необходимости профиль также может различать конфигурацию по trigger (например, запускать часть операций только на `generate` или только на `regenerate`).

### Run (запуск)

**Run** — конкретный запуск системы операций по **trigger** (пока: `generate` / `regenerate`).
Run — это “контейнер” для всех операций, которые отработали в рамках этого запуска.

### OperationRun (выполнение операции)

**OperationRun** — лог *конкретного выполнения* конкретной операции в рамках Run:

- когда стартовал/закончился,
- чем закончился (ok/error/aborted),
- какие были входы/выходы (в разумных лимитах),
- какие были ошибки (стабильный код + безопасное сообщение).

Важно: Operation (в профиле) — “что сделать”, OperationRun — “как это реально отработало в этот раз”.

### Main LLM (основная генерация)

**Main LLM** — это **core‑процесс основной генерации**, встроенный в систему чата и выполняющийся **по своему отдельному жизненному циклу**.
В рамках v2 мы **не управляем им напрямую** как обычной `Operation`: операции не “встраиваются внутрь” main LLM и не могут пошагово редактировать его выполнение.

Роль main LLM в `Run`:

- он является **разделительной границей** для хуков: операции могут выполняться **только `before_main_llm`** и **`after_main_llm`** относительно этого процесса;
- он использует **effective prompt**, подготовленный системой (chat history + синтетические вставки/артефакты/результаты операций), и запускает **основной LLM‑вызов**;
- результат main LLM — это **генерация ответа ассистента** (обычно со стримингом в UI) и **запись результата в историю чата** как новый/обновлённый **assistant variant** текущего хода.

На текущий момент в каждом `Run` выполняется **ровно один** main LLM‑вызов.
`Aux LLM` вызовы внутри операций **не связаны** с ним напрямую и **не влияют** на него “изнутри” — они могут повлиять на итог **только через свои результаты**, которые операции применяют **до** main LLM (например, меняя effective prompt / создавая артефакты для включения в prompt) или **после** main LLM (например, меняя UI/артефакты/варианты текущего хода).

### Aux LLM (вспомогательная генерация)

**Aux LLM** — LLM‑вызов операции, который:

- **не** стримит в chat history как “основной ответ ассистента”,
- используется, чтобы произвести данные/артефакты/варианты для main LLM или UI.

### Artifact (артефакт)

**Artifact** — результат работы операций, который живёт отдельно от истории сообщений.
У артефакта есть **две независимые оси**:

1) **Как используется (видимость / usage)** — *кто это видит и где применяется*:

- **prompt_only** — участвует только в effective prompt
- **ui_only** — показывается только в UI
- **prompt+ui** — и участвует в effective prompt, и показывается в UI
- **internal** — не участвует в effective prompt и не показывается в UI (нужен только для вычислений/связок между операциями)

2) **Что это по смыслу (kind / semantics)** — *что именно хранится*:

- **state** — состояние/память мира (погода/время/локация/статы и т.п.)
- **log/feed** — лента/журнал событий
- **lore/memory** — лор/память персонажей
- **intermediate** — промежуточный результат (в т.ч. guard-флаги, tool-выходы, кэш).  
  Обычно такой артефакт имеет usage=`internal`, но при необходимости может быть и `prompt_only` (если надо скормить main LLM) или `ui_only` (если надо показать отладку/индикатор).

Артефакт адресуется как `art.<tag>` и имеет минимум `value`.  

**Область видимости `tag`**: каждый `tag` должен быть **уникальным в рамках одного OperationProfile** (т.е. набора/“пайплайна” операций). Коллизия `tag` в одном профиле — это ошибка конфигурации.
Детали “persisted vs run_only” и “может ли быть history” — см. раздел **3.3** ниже.

#### Пример, чтобы не путаться

`art.world_state` может быть:

- **state** по смыслу (semantics)
- и **prompt+ui** по использованию (usage), чтобы и LLM, и пользователь видели текущее состояние.

### ChatHistory (история чата)

**ChatHistory** — канонический поток сообщений чата.  
В v2 сохраняем принцип: операции **не переписывают прошлую историю “задним числом”**.

Важно: `ChatHistory` — это история сообщений чата, а `history[]` у артефакта — это история прошлых состояний конкретного `art.<tag>` (см. **3.3**).

При этом операции могут влиять на:

- **текущий ход (turn)** (например, выбрать/создать variant для текущего сообщения),
- **effective prompt** (что уйдёт в main LLM),
- **артефакты** (память/панели/ленты/данные).

### Turn (ход)

**Turn (ход)** — это текущий “вопрос пользователя” (конкретный `userMessageId`) и все связанные с ним варианты ответа ассистента (**assistant variants**).

Правила:

- `trigger=generate` начинает **новый turn** (создаётся новый `userMessageId`).
- `trigger=regenerate` запускает новый `Run` **в рамках того же turn** и создаёт/добавляет новый **assistant variant** для текущего `userMessageId`.

### Variant (вариант сообщения)

**Variant** — альтернативная форма одного и того же сообщения (user или assistant), из которых выбирается “активная/выбранная” версия.
Это механизм “изменить текущий ход” без переписывания истории задним числом.

### Effective prompt (фактический промпт)

**Effective prompt** — то, что реально уходит в main LLM в этот раз:

- system prompt,
- часть chat history,
- синтетические вставки (например, из артефактов или результатов операций).

### PromptInclusion (включение артефакта в effective prompt)

**PromptInclusion** — это правило, которое описывает **как именно** артефакт (чаще всего persisted) попадает в effective prompt.

Минимальная идея:

- можно включать артефакт как “кусок system”,
- как отдельное синтетическое сообщение,
- или вставлять “после последнего user”,
- при необходимости — с более точным якорем/глубиной (см. примеры в **3.1 Prompt-time**).

## 3) Три слоя изменений (очень важная рамка)

Слои нужны, чтобы **не смешивать разные “виды правды”** и не получать “магии”.
Любой эффект операции должен быть отнесён к одному (или нескольким) из слоёв ниже.

### 3.1 Prompt-time (одноразово для effective prompt)

**Что это**: операция влияет **только на effective prompt текущего main LLM вызова**.  
История чата в БД при этом **не меняется**.

**Важно**: effective prompt — это не только system-инструкция, а **весь набор сообщений**, который реально уходит в main LLM:

- system prompt,
- выбранная chat history (promptText выбранных вариантов),
- синтетические сообщения/вставки (созданные операциями).

**Что можно делать** (примеры):

- сделать aux LLM вызов и вставить “working notes / план / augmentation” как `prompt_only` сообщение;
- сделать RAG и вложить найденный контекст в effective prompt (в system или “после последнего user”);
- вставить одноразовое сообщение **на глубину** (не только в конец), задав:
  - якорь (после последнего user / после messageId / offset от конца),
  - место (before/after/replace),
  - и политику, что делать, если якорь не попал в окно из‑за trimming.

### 3.2 Turn canonicalization (канонизация текущего хода)

**Что это**: операция меняет **канон текущего хода**, но **не переписывает прошлое**.

Идея простая: если ты хочешь, чтобы результат “жил дальше” в истории чата и использовался на следующих ходах,
его нужно закрепить как **selected variant** (или как blocks/meta) **текущего** сообщения.

**Что можно делать** (примеры):

- для текущего `userMessageId`: создать variant “перевод/переписанный голосом персонажа” и сделать его selected  
  → на следующем ходу chat history будет использовать именно этот текст (пока selection не переключат обратно);
- для текущего `assistantVariantId`: нормализовать ответ, записать blocks.

### 3.3 Artifacts / Memory (данные и состояние рядом с историей)

**Что это**: операция читает/пишет **артефакты** — отдельный слой данных, который живёт рядом с историей сообщений.

**Базовые правила (v2)**:

1) **Любой артефакт адресуется как `art.<tag>`** и имеет минимум `value`.
2) **persisted** артефакт **живёт между run** и доступен в следующих ходах; он **может иметь историю** (`history[]`) по retention.
3) **run_only** артефакт — **эфемерный результат исполнения**, который существует только *во время* одного run и **не сохраняется** между ходами; он **не может иметь историю**.

**Примеры persisted**:

- **State-панель (долговременная память мира)**: `art.world_state` (json) хранит время/погоду/локацию и обновляется после каждого хода. UI показывает `value`, а история версий (`history[]`) может использоваться для диффа/отладки.
- **Лор/память персонажей**: `art.lore` (markdown/text) живёт между ходами и включается в effective prompt через `promptInclusion` (например `prepend_system`).
- **Лента событий**: `art.combat_log` (markdown/text) хранит последние \(N\) записей как timeline (через retention) и отображается в UI как feed.

**Зачем нужен run_only**: чтобы передавать/кэшировать результаты **между операциями внутри одного запуска**, не превращая это в “память чата”.
Это снижает когнитивную нагрузку (не нужно думать “чистить ли это между ходами”), избегает “утечек” данных в следующий ход и позволяет держать большие/временные результаты без сохранения в БД.

**Примеры run_only**:

- **Guard-флаг для ветвления внутри run**: операция анализирует последние \(N\) сообщений и возвращает `isCombat=true/false` для этого запуска, чтобы далее включить/пропустить операции “дайсы/бой”. На следующий ход этот флаг не обязан жить.
- **Большой одноразовый контекст для main LLM**: RAG/aux LLM собирает 10k токенов справки/augmentation для *одного* ответа и отдаёт это как run_only результат (при желании логируется summary/hash), не сохраняя между ходами.
- **Промежуточный tool-результат/кэш**: операция сходила в API и вернула сырой JSON, который нужен только следующим операциям в этом же run (например для расчёта), но не нужен как долговременный артефакт.

**Важно**: артефакты — это про **хранение данных**, а то, **как это показывать в UI** (панель/лента/внутри чата) — это **отдельная проекция** и не равно retention.

---

Исторические трансформации (переписывание прошлого) — **отдельный класс операций** и по умолчанию не часть v2 core.

