# v2 — Operation / kind=llm (контракт LLM-операции)

Этот документ описывает **контракт и поведение операций** с `OperationDefinition.kind="llm"`.

> Важно: здесь речь про **aux LLM-операции** (внутри `Operation`).  
> `Main LLM` остаётся отдельным процессом `Run` и **не** описывается как обычная операция.

## 0) Цель и границы (scope)

**Цель**: зафиксировать минимальный, реализуемый контракт для aux LLM-операций, чтобы:

- можно было конфигурировать LLM-операции как обычные `Operation` в профиле,
- поведение было предсказуемым (timeouts/retry/output format),
- логирование было полезным и безопасным.

**Не в scope** этого документа:

- контракт `Main LLM` (это отдельный процесс `Run`);
- подробная схема `OperationEffects` и детальный “движок” применения эффектов (описывается отдельно; здесь только то, что LLM-операция может вернуть как эффекты).

## 1) Что такое LLM-операция в v2 (простыми словами)

**LLM-операция** — это операция, которая делает **ровно один aux LLM-вызов**, получает результат и возвращает его в виде:

- записи в **один** `art.<tag>` (persisted или run_only) — основной способ “выдать данные наружу”,
- и/или дополнительных эффектов (prompt-time / canonicalization), если операция на это рассчитана.

Инварианты:

- LLM-операция — **blackbox**: все внешние эффекты должны быть описаны явно через `OperationResult`.
- LLM-операция **не стримит** “основной ответ ассистента” в chat history (это делает `Main LLM`).
- LLM-операция в v2 **не “ведёт чат”**: она одноразово формирует запрос и выполняется ровно один раз в рамках своего `OperationRun`.

## 2) `params` для `kind="llm"` (базовая структура)

В v2 `OperationConfig.params` для `kind="llm"` — JSON-объект со следующими логическими группами.
Точная типизация будет вынесена в shared types позже, здесь фиксируем форму/смысл.

### 2.1 Provider / Model

- `providerRef: string` — ссылка на провайдера/интеграцию (без секретов; ключи живут вне params)
- `model: string` — имя модели (как понимает провайдер)
- `credentialRef: string` — ссылка на токен/учётку в DB, которая будет использована во время `Run` (**никогда не логируется** и не сохраняется “как есть” в `OperationRun`; допустимо логировать только факт выбора провайдера/credentialRef как opaque id при включённом debug, если нужно).

Примечание (не обязательно для v2 core): в будущем можно добавить `modelRef` (идентификатор модели из каталога приложения) вместо/вместе с `model`, но это не требуется для рабочего минимума.

### 2.2 Prompt (что отправляем в aux LLM)

В v2 (рабочий минимум) LLM-операция — **one-shot**: она отправляет один запрос, который концептуально состоит из:

- `system?: string` — опциональная системная инструкция для aux вызова
- `prompt: string` — “user message” для aux вызова (ровно одна строка)

Оба поля (`system` и `prompt`) являются **LiquidJS-шаблонами** и рендерятся в строку перед выполнением aux LLM-вызова.
В шаблоне доступны, как минимум:

- `art.<tag>` — все артефакты (`art.stats`, `art.world_state`, и т.п.)
- `chatHistory` — история чата (с фильтрами/хелперами Liquid, например `{{ chatHistory | last: 5 }}` — точная форма фильтров фиксируется отдельно)

Примечание про “наличие данных”:

- наличие/отсутствие конкретных `art.<tag>` — часть ответственности пользователя при настройке профиля/связей операций;
- система должна быть прозрачной: указывать, **что** доступно в контексте и **как** это настроить;
- если пользователю нужна гарантия, что run-local артефакт появится “к моменту рендера”, он настраивает связи через `dependsOn` (см. v2 модель зависимостей).

Опционально:

- `strictVariables?: boolean` — режим строгих переменных при рендере LiquidJS.
  - если `true`: обращение к отсутствующему полю/переменной при рендере считается ошибкой и операция завершается `status="error"` (например `error.code="template_render_error"`).
  - если `false`/не задано: поведение определяется настройками Liquid по умолчанию (обычно “рендерим пусто”).
- `maxInputTokens?: number` — мягкий лимит на вход (если поддерживается)

> Примечание: в будущем это можно расширить до `messages[]` или `promptTemplateRef + vars`, но в v2 core фиксируем one-shot модель, чтобы не создавать “второй чат внутри операции”.

### 2.3 Sampling / Limits

- `samplers?: { temperature?: number; topP?: number; topK?: number; frequencyPenalty?: number; presencePenalty?: number; seed?: number }`
- `maxOutputTokens?: number`
- `stop?: string[]`
- `budget?: { maxCostUsd?: number }`

### 2.4 Output / Parsing

- `output?: { mode: "text" | "json"; schemaRef?: string }`
  - `mode="text"` — ожидаем обычный текст
  - `mode="json"` — ожидаем JSON; `schemaRef` опционально (валидация по схеме)

`mode="json"` — **опциональный best-effort режим**, который зависит от провайдера/модели и корректности промпта. Это ответственность пользователя, а не системы: система не гарантирует, что модель вернёт валидный JSON.

Политика v2:

- если `mode="json"` и результат **не парсится** как JSON → операция завершаетcя `status="error"` (например `error.code="output_parse_error"`).
- если операция `required=true` и находится в `before_main_llm`, то этот `error` **останавливает Run** до main LLM (см. барьер в модели Operation).

В UI можно будет “проверить схему/валидность” (`schemaRef`), но механика валидации описывается отдельным документом.

### 2.5 Куда записать результат (обязательная часть v2)

Так как действует правило v2 “одна операция пишет только в один `art.<tag>`”, LLM-операция должна явно знать, куда писать результат.

- `writeArtifact: { tag: string; persisted: boolean; usage: "prompt_only" | "ui_only" | "prompt+ui" | "internal"; semantics: "state" | "log/feed" | "lore/memory" | "intermediate" | string }`

Примечание: `retention` для persisted артефактов описывается в спеках артефактов/памяти, а не в контракте `kind="llm"`.

### 2.6 Поведение выполнения

- `timeoutMs?: number`
- `retry?: { maxAttempts: number; backoffMs?: number; retryOn?: ("timeout" | "provider_error" | "rate_limit")[] }`

## 3) Входы (`OperationContext`) и ограничения

LLM-операция читает из `OperationContext`:

- базовую meta (`runId`, `hook`, `trigger`, `chatId`, `turnId/userMessageId`, и т.п.)
- `art.<tag>` (persisted и/или run-local)  
  (если нужно гарантировать наличие run-local результата — обеспечиваем через `dependsOn`)
- `abortSignal` / `deadlineMs?`

Ограничения:

- LLM-операция не должна утекать секретами в логи/артефакты.
- В `before_main_llm` LLM-операция может влиять на effective prompt только через **prompt-time эффекты** (и только пока не прошёл барьер).

Примечание: синтаксис LiquidJS, фильтры и полный список доступных переменных (включая `chatHistory`) **не являются зоной ответственности этого документа** и описываются отдельно.

## 4) Выходы (`OperationResult`) и эффекты

### 4.1 Минимальный результат

LLM-операция возвращает:

- `status: "done" | "skipped" | "error" | "aborted"`
- `effects` (обычно включает запись `art.<tag>` по `params.writeArtifact`)

### 4.2 Что считается “результатом” LLM-операции

Минимально: значение, которое будет записано в артефакт:

- при `output.mode="text"`: `value` = строка
- при `output.mode="json"`: `value` = объект (если распарсилось), иначе `status="error"` по политике `output_parse_error` (см. раздел 2.4)

Опционально (bounded, для дебага):

- `debug.providerRequestSummary` (без секретов)
- `debug.providerResponseSummary` (без больших/чувствительных payload)

Для `mode="json"` сохраняем `rawText` (bounded) как часть debug/summary, чтобы было видно, почему JSON не распарсился.
При этом “сырой ответ” не должен логироваться без ограничений: сохраняем только безопасный/усечённый вариант и/или хэш.

## 6.1 Рекомендация: поля для `OperationRun.inputsSummary/outputsSummary` (LLM)

Ниже — рекомендуемый набор полей для объяснимого дебага. Все поля должны быть **bounded** (обрезка, лимиты) и без секретов.

`inputsSummary` (минимум):

- `providerRef`, `model`
- `outputMode` (`text|json`), `schemaRef?`
- `samplers` (только численные параметры), `maxOutputTokens?`, `stop?`
- `timeoutMs?`, `retry?` (только конфиг)
- `strictVariables?`
- `renderedSystemHash?`, `renderedPromptHash?` (хэш итоговых строк после Liquid render, чтобы понимать “что было отправлено” без логирования текста)

`outputsSummary` (минимум):

- `attempts` (сколько попыток реально сделали с учётом retry)
- `durationMs`
- `providerRequestId?` / `providerTraceId?` (если провайдер возвращает)
- `usage?` (если провайдер возвращает): `inputTokens?`, `outputTokens?`, `totalTokens?`, `costUsd?`
- `finishReason?`
- при `mode="json"`:
  - `rawTextPreview?` (усечённый текст ответа, например первые N символов)
  - `rawTextHash?`
  - `parseErrorMessage?` (кратко, bounded), если парсинг упал

## 5) Ошибки, ретраи, таймауты

Базовая идея:

- таймаут по `timeoutMs` / `deadlineMs` → `status="error"` (или `aborted`, если это отмена пользователем)
- при `retry.maxAttempts>1` делаем повтор по `retryOn`

Рекомендованные коды ошибок (черновик):

- `timeout`
- `rate_limited`
- `provider_error`
- `output_parse_error`
- `budget_exceeded`
- `policy_error`

## 6) Логирование и безопасность

Минимальные требования:

- не логировать “сырой prompt” и “сырой ответ” без явного `debug.enabled` и лимитов
- всегда редактировать/обрезать большие поля (bounded summaries)
- не хранить secrets в `params` и в `debug`

Рекомендуемые лимиты для bounded summaries (можно менять реализацией, но это хороший дефолт):

- **`rawTextPreview`**: первые **1024** символа
- **`parseErrorMessage`**: первые **512** символов
- **`stop[]`**: максимум **10** элементов, каждый максимум **120** символов (в summary)
- **`samplers`**: только численные значения; без “производных” полей
- **`renderedSystemHash` / `renderedPromptHash`**: всегда можно логировать (это хэш, не контент)

Redaction (минимум):

- маскировать строки, похожие на ключи/токены (по простым эвристикам)
- никогда не логировать `credentialRef` в явном виде, если он может быть чувствительным; если нужен дебаг — логировать только opaque id при включённом debug

## 7) Открытые вопросы (чтобы дополнять вместе)

На данный момент — нет критичных открытых вопросов для рабочего минимума `kind="llm"`.

